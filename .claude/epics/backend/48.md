---
name: 搜索引擎集成
status: backlog
created: 2025-09-15T02:51:00Z
github: https://github.com/rcnn/sharestack/issues/48
depends_on: [34, 35]
parallel: true
conflicts_with: []
---

# Task: 搜索引擎集成

## Description

实现高性能搜索引擎集成系统，包括Elasticsearch集成、全文搜索、搜索索引管理、搜索结果排序、搜索统计分析等功能。为平台提供专业级的搜索体验。

## Acceptance Criteria

- [ ] 实现Elasticsearch集成
- [ ] 实现全文搜索引擎配置
- [ ] 实现搜索索引建立和维护
- [ ] 实现搜索结果排序优化
- [ ] 实现搜索统计分析
- [ ] 实现搜索建议和自动完成
- [ ] 实现搜索性能监控
- [ ] 编写完整的测试用例

## Technical Details

### Elasticsearch集成架构

#### 搜索引擎配置
```python
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import json

class ElasticsearchService:
    def __init__(self):
        self.es = Elasticsearch(
            hosts=[settings.ELASTICSEARCH_HOST],
            http_auth=(settings.ELASTICSEARCH_USER, settings.ELASTICSEARCH_PASSWORD),
            timeout=30,
            max_retries=3,
            retry_on_timeout=True
        )

        # 索引配置
        self.indices = {
            'contents': 'sharestack_contents',
            'users': 'sharestack_users',
            'tags': 'sharestack_tags'
        }

    def create_indices(self):
        """创建搜索索引"""
        # 内容索引
        content_mapping = {
            "mappings": {
                "properties": {
                    "id": {"type": "long"},
                    "title": {
                        "type": "text",
                        "analyzer": "ik_smart",
                        "search_analyzer": "ik_smart",
                        "fields": {
                            "keyword": {"type": "keyword"}
                        }
                    },
                    "content": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    "summary": {
                        "type": "text",
                        "analyzer": "ik_smart"
                    },
                    "author": {
                        "properties": {
                            "id": {"type": "long"},
                            "username": {"type": "keyword"},
                            "nickname": {"type": "text", "analyzer": "ik_smart"}
                        }
                    },
                    "tags": {"type": "keyword"},
                    "categories": {"type": "keyword"},
                    "content_type": {"type": "keyword"},
                    "status": {"type": "keyword"},
                    "is_paid": {"type": "boolean"},
                    "price": {"type": "float"},
                    "view_count": {"type": "long"},
                    "like_count": {"type": "long"},
                    "comment_count": {"type": "long"},
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"},
                    "published_at": {"type": "date"},
                    "suggest": {
                        "type": "completion",
                        "analyzer": "simple",
                        "preserve_separators": True,
                        "preserve_position_increments": True,
                        "max_input_length": 50
                    }
                }
            },
            "settings": {
                "number_of_shards": 3,
                "number_of_replicas": 1,
                "analysis": {
                    "analyzer": {
                        "ik_smart": {
                            "type": "ik_smart"
                        },
                        "ik_max_word": {
                            "type": "ik_max_word"
                        }
                    }
                }
            }
        }

        self._create_index_if_not_exists('contents', content_mapping)

        # 用户索引
        user_mapping = {
            "mappings": {
                "properties": {
                    "id": {"type": "long"},
                    "username": {"type": "keyword"},
                    "nickname": {
                        "type": "text",
                        "analyzer": "ik_smart",
                        "fields": {"keyword": {"type": "keyword"}}
                    },
                    "bio": {"type": "text", "analyzer": "ik_smart"},
                    "follower_count": {"type": "long"},
                    "content_count": {"type": "long"},
                    "is_verified": {"type": "boolean"},
                    "created_at": {"type": "date"},
                    "suggest": {
                        "type": "completion",
                        "analyzer": "simple"
                    }
                }
            }
        }

        self._create_index_if_not_exists('users', user_mapping)

    def _create_index_if_not_exists(self, index_type, mapping):
        """创建索引（如果不存在）"""
        index_name = self.indices[index_type]

        if not self.es.indices.exists(index=index_name):
            self.es.indices.create(index=index_name, body=mapping)
            logger.info(f"创建索引: {index_name}")
        else:
            logger.info(f"索引已存在: {index_name}")
```

### 搜索索引管理

#### 内容索引同步
```python
class SearchIndexManager:
    def __init__(self):
        self.es_service = ElasticsearchService()

    def index_content(self, content):
        """索引单个内容"""
        doc = self._prepare_content_doc(content)

        try:
            self.es_service.es.index(
                index=self.es_service.indices['contents'],
                id=content.id,
                body=doc
            )
            logger.info(f"内容索引成功: {content.id}")
        except Exception as e:
            logger.error(f"内容索引失败: {content.id}, 错误: {str(e)}")

    def bulk_index_contents(self, contents):
        """批量索引内容"""
        actions = []

        for content in contents:
            doc = self._prepare_content_doc(content)
            action = {
                "_index": self.es_service.indices['contents'],
                "_id": content.id,
                "_source": doc
            }
            actions.append(action)

        try:
            success, failed = bulk(self.es_service.es, actions)
            logger.info(f"批量索引完成: 成功{success}, 失败{len(failed)}")
            return success, failed
        except Exception as e:
            logger.error(f"批量索引失败: {str(e)}")
            return 0, len(actions)

    def _prepare_content_doc(self, content):
        """准备内容文档"""
        # 提取标签和分类
        tags = [tag.name for tag in content.tags.all()] if hasattr(content, 'tags') else []
        categories = [cat.name for cat in content.categories.all()] if hasattr(content, 'categories') else []

        # 生成搜索建议
        suggest_input = [content.title]
        if tags:
            suggest_input.extend(tags)

        doc = {
            "id": content.id,
            "title": content.title,
            "content": content.content,
            "summary": content.summary or "",
            "author": {
                "id": content.author.id,
                "username": content.author.username,
                "nickname": getattr(content.author, 'nickname', content.author.username)
            },
            "tags": tags,
            "categories": categories,
            "content_type": content.content_type,
            "status": content.status,
            "is_paid": content.is_paid,
            "price": float(content.price) if content.price else 0.0,
            "view_count": getattr(content, 'view_count', 0),
            "like_count": getattr(content, 'like_count', 0),
            "comment_count": getattr(content, 'comment_count', 0),
            "created_at": content.created_at.isoformat(),
            "updated_at": content.updated_at.isoformat(),
            "published_at": content.published_at.isoformat() if content.published_at else None,
            "suggest": {
                "input": suggest_input,
                "weight": self._calculate_content_weight(content)
            }
        }

        return doc

    def _calculate_content_weight(self, content):
        """计算内容权重（用于搜索排序）"""
        weight = 1

        # 基于浏览量
        weight += min(getattr(content, 'view_count', 0) // 100, 50)

        # 基于点赞数
        weight += min(getattr(content, 'like_count', 0) // 10, 30)

        # 基于评论数
        weight += min(getattr(content, 'comment_count', 0) // 5, 20)

        # 基于作者等级
        if hasattr(content.author, 'is_verified') and content.author.is_verified:
            weight += 10

        # 基于内容新鲜度
        days_old = (datetime.now() - content.created_at).days
        if days_old < 7:
            weight += 15
        elif days_old < 30:
            weight += 10
        elif days_old < 90:
            weight += 5

        return weight

    def delete_content_index(self, content_id):
        """删除内容索引"""
        try:
            self.es_service.es.delete(
                index=self.es_service.indices['contents'],
                id=content_id
            )
            logger.info(f"删除内容索引: {content_id}")
        except Exception as e:
            logger.error(f"删除内容索引失败: {content_id}, 错误: {str(e)}")

    @celery.task
    def rebuild_search_index():
        """重建搜索索引"""
        manager = SearchIndexManager()

        # 删除现有索引
        for index_name in manager.es_service.indices.values():
            if manager.es_service.es.indices.exists(index=index_name):
                manager.es_service.es.indices.delete(index=index_name)

        # 重新创建索引
        manager.es_service.create_indices()

        # 批量索引所有内容
        contents = ContentRepository.get_all_published()
        batch_size = 1000

        for i in range(0, len(contents), batch_size):
            batch = contents[i:i + batch_size]
            manager.bulk_index_contents(batch)

        logger.info("搜索索引重建完成")
```

### 高级搜索功能

#### 搜索查询构建器
```python
class SearchQueryBuilder:
    def __init__(self):
        self.es_service = ElasticsearchService()

    def build_search_query(self, search_params):
        """构建搜索查询"""
        query = {
            "query": {
                "bool": {
                    "must": [],
                    "filter": [],
                    "should": [],
                    "must_not": []
                }
            },
            "sort": [],
            "highlight": {
                "fields": {
                    "title": {},
                    "content": {},
                    "summary": {}
                },
                "pre_tags": ["<mark>"],
                "post_tags": ["</mark>"]
            },
            "aggs": {
                "categories": {
                    "terms": {"field": "categories", "size": 20}
                },
                "tags": {
                    "terms": {"field": "tags", "size": 30}
                },
                "content_types": {
                    "terms": {"field": "content_type", "size": 10}
                },
                "price_ranges": {
                    "range": {
                        "field": "price",
                        "ranges": [
                            {"key": "free", "to": 0.01},
                            {"key": "low", "from": 0.01, "to": 10},
                            {"key": "medium", "from": 10, "to": 50},
                            {"key": "high", "from": 50}
                        ]
                    }
                }
            }
        }

        # 主搜索查询
        if search_params.get('q'):
            main_query = self._build_main_query(search_params['q'])
            query["query"]["bool"]["must"].append(main_query)

        # 过滤条件
        self._add_filters(query, search_params)

        # 排序
        self._add_sorting(query, search_params)

        # 分页
        page = search_params.get('page', 1)
        page_size = min(search_params.get('page_size', 20), 100)
        query["from"] = (page - 1) * page_size
        query["size"] = page_size

        return query

    def _build_main_query(self, search_text):
        """构建主搜索查询"""
        return {
            "multi_match": {
                "query": search_text,
                "fields": [
                    "title^3",         # 标题权重最高
                    "summary^2",       # 摘要权重次之
                    "content",         # 内容权重正常
                    "author.nickname^2", # 作者昵称权重较高
                    "tags^2"           # 标签权重较高
                ],
                "type": "best_fields",
                "fuzziness": "AUTO",
                "operator": "and"
            }
        }

    def _add_filters(self, query, params):
        """添加过滤条件"""
        # 内容状态过滤
        query["query"]["bool"]["filter"].append({
            "term": {"status": "published"}
        })

        # 内容类型过滤
        if params.get('content_type'):
            query["query"]["bool"]["filter"].append({
                "term": {"content_type": params['content_type']}
            })

        # 分类过滤
        if params.get('categories'):
            categories = params['categories'] if isinstance(params['categories'], list) else [params['categories']]
            query["query"]["bool"]["filter"].append({
                "terms": {"categories": categories}
            })

        # 标签过滤
        if params.get('tags'):
            tags = params['tags'] if isinstance(params['tags'], list) else [params['tags']]
            query["query"]["bool"]["filter"].append({
                "terms": {"tags": tags}
            })

        # 作者过滤
        if params.get('author_id'):
            query["query"]["bool"]["filter"].append({
                "term": {"author.id": params['author_id']}
            })

        # 价格过滤
        if params.get('is_free'):
            query["query"]["bool"]["filter"].append({
                "range": {"price": {"lte": 0}}
            })
        elif params.get('price_range'):
            price_range = params['price_range']
            range_filter = {"range": {"price": {}}}
            if 'min' in price_range:
                range_filter["range"]["price"]["gte"] = price_range['min']
            if 'max' in price_range:
                range_filter["range"]["price"]["lte"] = price_range['max']
            query["query"]["bool"]["filter"].append(range_filter)

        # 时间范围过滤
        if params.get('date_range'):
            date_range = params['date_range']
            range_filter = {"range": {"published_at": {}}}
            if 'start' in date_range:
                range_filter["range"]["published_at"]["gte"] = date_range['start']
            if 'end' in date_range:
                range_filter["range"]["published_at"]["lte"] = date_range['end']
            query["query"]["bool"]["filter"].append(range_filter)

    def _add_sorting(self, query, params):
        """添加排序"""
        sort_by = params.get('sort_by', 'relevance')
        sort_order = params.get('sort_order', 'desc')

        if sort_by == 'relevance':
            # 相关性排序（默认）
            query["sort"].append({"_score": {"order": "desc"}})
        elif sort_by == 'date':
            query["sort"].append({"published_at": {"order": sort_order}})
        elif sort_by == 'popularity':
            query["sort"].append({"view_count": {"order": sort_order}})
        elif sort_by == 'likes':
            query["sort"].append({"like_count": {"order": sort_order}})
        elif sort_by == 'price':
            query["sort"].append({"price": {"order": sort_order}})

        # 二级排序：总是按相关性
        if sort_by != 'relevance':
            query["sort"].append({"_score": {"order": "desc"}})
```

### 搜索建议和自动完成

#### 搜索建议服务
```python
class SearchSuggestionService:
    def __init__(self):
        self.es_service = ElasticsearchService()

    def get_suggestions(self, query, limit=10):
        """获取搜索建议"""
        if len(query) < 2:
            return []

        # 使用completion suggester
        suggest_query = {
            "suggest": {
                "content_suggest": {
                    "prefix": query,
                    "completion": {
                        "field": "suggest",
                        "size": limit,
                        "skip_duplicates": True
                    }
                }
            }
        }

        try:
            response = self.es_service.es.search(
                index=self.es_service.indices['contents'],
                body=suggest_query
            )

            suggestions = []
            for suggestion in response['suggest']['content_suggest'][0]['options']:
                suggestions.append({
                    'text': suggestion['text'],
                    'score': suggestion['_score'],
                    'source': suggestion['_source']
                })

            return suggestions

        except Exception as e:
            logger.error(f"获取搜索建议失败: {str(e)}")
            return []

    def get_related_queries(self, query, limit=5):
        """获取相关搜索"""
        # 查找相似的搜索记录
        related_query = {
            "query": {
                "more_like_this": {
                    "fields": ["title", "content", "tags"],
                    "like": query,
                    "min_term_freq": 1,
                    "max_query_terms": 12
                }
            },
            "size": limit,
            "_source": ["title", "tags"]
        }

        try:
            response = self.es_service.es.search(
                index=self.es_service.indices['contents'],
                body=related_query
            )

            related_queries = []
            for hit in response['hits']['hits']:
                source = hit['_source']
                # 生成相关查询建议
                related_queries.append(source['title'])
                if source.get('tags'):
                    related_queries.extend(source['tags'][:2])

            # 去重并返回
            return list(set(related_queries))[:limit]

        except Exception as e:
            logger.error(f"获取相关搜索失败: {str(e)}")
            return []

    def get_popular_searches(self, limit=10):
        """获取热门搜索"""
        # 从搜索统计中获取热门关键词
        popular_searches = SearchStatsRepository.get_popular_keywords(limit)
        return [item['keyword'] for item in popular_searches]
```

### 搜索统计和分析

#### 搜索统计服务
```python
class SearchAnalyticsService:
    def record_search(self, query, user_id=None, results_count=0, search_time=0):
        """记录搜索行为"""
        search_record = {
            'query': query,
            'user_id': user_id,
            'results_count': results_count,
            'search_time': search_time,
            'timestamp': datetime.now(),
            'date': datetime.now().date()
        }

        # 异步记录搜索统计
        self._record_search_async.delay(search_record)

    @celery.task
    def _record_search_async(self, search_record):
        """异步记录搜索统计"""
        try:
            # 记录到数据库
            SearchLogRepository.create(search_record)

            # 更新搜索关键词统计
            self._update_keyword_stats(search_record['query'])

            # 更新用户搜索行为统计
            if search_record['user_id']:
                self._update_user_search_stats(search_record['user_id'], search_record)

        except Exception as e:
            logger.error(f"记录搜索统计失败: {str(e)}")

    def _update_keyword_stats(self, query):
        """更新关键词统计"""
        # 分词提取关键词
        keywords = self._extract_keywords(query)

        for keyword in keywords:
            try:
                stats = SearchKeywordStatsRepository.get_or_create(keyword)
                stats.search_count += 1
                stats.last_searched_at = datetime.now()
                SearchKeywordStatsRepository.update(stats)
            except Exception as e:
                logger.error(f"更新关键词统计失败: {keyword}, {str(e)}")

    def _extract_keywords(self, query):
        """提取搜索关键词"""
        # 使用jieba分词
        import jieba

        # 去除停用词
        stop_words = set(['的', '了', '在', '是', '我', '有', '和', '就', '不', '人', '都', '一', '一个'])

        keywords = []
        words = jieba.cut(query)

        for word in words:
            word = word.strip()
            if len(word) > 1 and word not in stop_words:
                keywords.append(word)

        return keywords

    def get_search_analytics(self, date_range=None):
        """获取搜索分析数据"""
        analytics = {
            'total_searches': 0,
            'unique_queries': 0,
            'avg_results_count': 0,
            'avg_search_time': 0,
            'popular_keywords': [],
            'search_trends': [],
            'no_results_queries': []
        }

        # 设置日期范围
        if not date_range:
            end_date = datetime.now().date()
            start_date = end_date - timedelta(days=30)
        else:
            start_date = date_range['start']
            end_date = date_range['end']

        # 获取基础统计
        basic_stats = SearchLogRepository.get_stats_by_date_range(start_date, end_date)
        analytics.update(basic_stats)

        # 获取热门关键词
        analytics['popular_keywords'] = SearchKeywordStatsRepository.get_popular_keywords(20)

        # 获取搜索趋势
        analytics['search_trends'] = self._get_search_trends(start_date, end_date)

        # 获取无结果查询
        analytics['no_results_queries'] = SearchLogRepository.get_no_results_queries(10)

        return analytics

    def _get_search_trends(self, start_date, end_date):
        """获取搜索趋势"""
        trends = []
        current_date = start_date

        while current_date <= end_date:
            day_stats = SearchLogRepository.get_stats_by_date(current_date)
            trends.append({
                'date': current_date.isoformat(),
                'search_count': day_stats['total_searches'],
                'unique_queries': day_stats['unique_queries']
            })
            current_date += timedelta(days=1)

        return trends
```

### 搜索性能监控

#### 搜索性能服务
```python
class SearchPerformanceMonitor:
    def __init__(self):
        self.es_service = ElasticsearchService()

    def monitor_search_performance(self, query_params, execution_time):
        """监控搜索性能"""
        performance_data = {
            'query_complexity': self._calculate_query_complexity(query_params),
            'execution_time': execution_time,
            'timestamp': datetime.now()
        }

        # 记录性能数据
        self._record_performance.delay(performance_data)

        # 检查性能阈值
        if execution_time > 2.0:  # 超过2秒认为是慢查询
            self._alert_slow_query(query_params, execution_time)

    def _calculate_query_complexity(self, params):
        """计算查询复杂度"""
        complexity = 1

        # 基于过滤条件数量
        filter_count = len([k for k in params.keys()
                           if k in ['categories', 'tags', 'content_type', 'author_id']])
        complexity += filter_count * 0.5

        # 基于搜索文本长度
        query_text = params.get('q', '')
        complexity += len(query_text.split()) * 0.1

        # 基于分页参数
        page_size = params.get('page_size', 20)
        complexity += page_size * 0.01

        return complexity

    @celery.task
    def _record_performance(self, performance_data):
        """记录性能数据"""
        try:
            SearchPerformanceRepository.create(performance_data)
        except Exception as e:
            logger.error(f"记录搜索性能失败: {str(e)}")

    def _alert_slow_query(self, params, execution_time):
        """慢查询告警"""
        alert_data = {
            'type': 'slow_search_query',
            'execution_time': execution_time,
            'query_params': params,
            'timestamp': datetime.now()
        }

        # 发送告警通知
        AlertService.send_alert(alert_data)

    def get_performance_report(self, date_range=None):
        """获取性能报告"""
        if not date_range:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=7)
        else:
            start_date = date_range['start']
            end_date = date_range['end']

        report = {
            'avg_execution_time': 0,
            'slow_queries_count': 0,
            'performance_trend': [],
            'bottlenecks': []
        }

        # 获取性能统计
        performance_stats = SearchPerformanceRepository.get_stats_by_date_range(
            start_date, end_date
        )

        report.update(performance_stats)

        return report
```

## Dependencies

- [ ] Task 013: 内容核心CRUD系统
- [ ] Task 014: 内容列表和搜索系统
- [ ] Elasticsearch搜索引擎
- [ ] IK中文分词插件
- [ ] Celery异步任务处理

## Effort Estimate

- Size: XL
- Hours: 30小时
- Parallel: true

## Definition of Done

- [ ] Elasticsearch集成完成并正常运行
- [ ] 全文搜索功能准确高效
- [ ] 搜索索引管理系统稳定
- [ ] 搜索结果排序优化有效
- [ ] 搜索建议和自动完成正常
- [ ] 搜索统计分析完整
- [ ] 搜索性能满足要求 (<500ms)
- [ ] 单元测试覆盖率达到90%
- [ ] API文档更新完成

## Implementation Notes

### Service层设计
```python
class AdvancedSearchService:
    def search(self, params, user_id=None):
        """执行搜索"""
        pass

    def suggest(self, query, limit=10):
        """获取搜索建议"""
        pass

    def get_analytics(self, date_range=None):
        """获取搜索分析"""
        pass

    def reindex_content(self, content_id):
        """重新索引内容"""
        pass
```

### 缓存策略
- `search:results:{hash}`: 搜索结果缓存 (TTL: 15分钟)
- `search:suggestions:{query}`: 搜索建议缓存 (TTL: 1小时)
- `search:popular`: 热门搜索缓存 (TTL: 30分钟)

### 性能优化
- 搜索结果缓存
- 索引预热策略
- 分布式搜索架构
- 搜索请求限流

### 错误处理
- 500: 搜索引擎不可用
- 400: 搜索参数无效
- 429: 搜索频率过高
- 503: 搜索服务过载

### 监控指标
- 搜索响应时间
- 搜索成功率
- 索引更新延迟
- 搜索引擎集群状态